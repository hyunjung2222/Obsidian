>[!NOTE] 트랜스포터 아키텍처란?
>RNN에 비해 성능과 속도 면에서 큰 폭으로 앞섰음
>확장 용이, 병렬 연산이 가능, 더 긴 입력 처리

[코드_코랩](https://colab.research.google.com/drive/1FHhBQKuL_uhS259Xqis-0eV13ZQ2zWv2#scrollTo=oHVobEq8xS0j)

- **셀프 어텐션**: 입력된 문장 내의 각 단어가 서로 어떤 관련이 있는지 계산해서 각 단어의 표현을 조정하는 역할
## 텍스트를 임베딩으로 변환하기
- 임베딩으로 변환하기 위한 세 가지 과정
1. 텍스트를 적절한 단위로 잘라 숫자형 아이디를 부여하는 **토큰화** 수행
2. 토큰 아이디를 토큰 임베딩 층을 통해 여러 숫자의 집합인 **토큰 임베딩**으로 변환
3. 위치 인코딩 층을 통해 토큰의 위치 정보를 담고 있는 **위치 임베딩**을 추가해 최종적으로 모델에 입력할 임베딩을 만듦
### 토큰화
- 서브워드 토큰화 방식: 자주 나오는 단어는 단어 단위 그대로 유지하고 가끔 나오는 단어는 더 작은 단위로 나눠 텍스트의 의미를 퇴대한 유지하면서도 사전의 크기는 작교 효율적으로 유지할 수 있음
### 토큰 임베딩으로 변환하기
- 앞선 토큰화에서 부여한 토큰 아이디는 하나의 숫자일 뿐, 토큰의 의미를 담을 수 없음
- 의미를 담기 위해서는 최소 2개 이상의 숫자 집합인 **벡터**여야 함
- 데이터를 의미를 담아 숫자 집합으로 변환하는 것: **임베딩**
- -> 여기서는 토큰을 임베딩으로 변환하기 때문에 **토큰 임베딩**이라고 부름
- 파이토치가 제공하는 ``nn.Embedding`` 클래스를 사용하면 토큰 아이디를 토큰 임베딩으로 변환할 수 있음
- 딥러닝에서는 모델이 특정 작업을 잘 수행하도록 학습하는 과정에서 데이터의 의미를 잘 담은 임베딩을 만드는 방법도 함께 학습함.
### 위치 인코딩
- 트랜스포머는 순차적인 처리 방식을 버리고 모든 입력을 동시에 처리하는데 , 그 과정에서 순서 정보가 사라지게 됨
- 텍스트에서 순서는 매우 중요한 정보이기 때문에, 그 역할을 **위치 인코딩**이 담당
### 멀티 헤드 어텐션
- 여러 어텐션 연산을 동시에 적용하면 성능을 더 높일 수 있음
- 토큰 사이의 관계를 한 가지 측면에서 이해하는 것보다 여러 측면을 동시에 고려할 때 언어나 문장에 대한 이해도가 높아질 것이다

## 정규화
> **정규화**: 딥러닝 모델에서 입력이 일정한 분포를 갖도록 만들어 학습이 안정적이고 빨라질 수 있도록 하는 기법

- 트랜스포머 아키텍처에서는 특징 차원에서 정규화를 수행하는 **층 정규화**를 사용
- 어텐션 연산이 입력 단어 사이의 관계를 계산해 토큰 임베딩을 조정하는 역할을 한다면 전체 입력 문장을 이해하는 연산도 필요, 트랜스포머 아키텍처에서는 이를 위해 **피드 포워드 층**을 사용
- **피드 포워드 층**: 데이터의 특징을 학습하는 완전 연결 층을 말함.
	- 멀티 헤드 어텐션: 단어 사이의 관계를 파악하는 역할
	- 피드 포워드 층: 입력 테스트 전체를 이해하는 역할

## 인코더
- 잔차 연결: 안정적인 학습이 가능하도록 도와줌. 화살표 모양 그대로 입력을 다시 더해주는 형태로 구현

## 디코더
1. 인코더는 멀티 헤드 어텐션을 사용하지만, 디코더 블록에서는 **마스크 멀티 헤드 어텐션**을 사용
	- 디코더는 생성을 담당하는 부분
	- 트랜스포머 모델도 사람처럼 앞에서 생성한 토큰을 기반으로 다음 토큰을 생성함
	- 어텐션을 그대로 활용할 경우 미래 시점에 작성해야하는 텍스트를 미리 확인하게 되는 문제가 생김
	- 이를 막기 위해 특정 시점에는 그 이전에 생성된 토큰까지만 확인할 수 있도록 마스크를 추가함
2. **크로스 어텐션**이 있음

## 트랜스포머를 활용한 아키텍처
1. 인코더만 활용해 자연어 이해 작업에 집중한 그룹
	1. BERT: 양방향 이해를 통해 자연어 이해에서 디코더 모델 대비 높은 성능, 자연어 생성 작업에 부적합한 형태
2. 디코더만 활용해 자연어 생성 작업에 집중한 그룹
	1. GPT: 생성 작업에서 뛰어난 성능, 단방향 방식이므로 자연어 이해 작업에서 비교적 성능이 낮음
3. 인코더와 디코더를 모두 활용해 더 넓은 범위의 작업을 수행할 수 있도록 한 그룹
	1. BART, T5: 생성과 이해 작업 모두에서 뛰어난 성능, 문맥을 반영한 생성 능력이 뛰어남, 더 복잡함, 학습에 더 많은 데이터와 컴퓨팅 자원이 필요함
# 모델 학습
1. 허깅페이스 트랜스포머에서 제공하는 트레이너 API
	1. 장점: 학습을 간편하게 할 수 있다
	2. 내부에서 어떤 과정을 거치는지 알기 어렵다.
2. Trainer를 사용하지 않고 직접 학습하기
	1. 장점: 내부 동작을 명확히 확인할 수 있고 직접 학습 과정을 조절할 수 있다.

- 학습한 모델은 허깅페이스 허브에 업로드할 수 있음.

# 모델 추론
1. 추상화된 파이프라인을 활용하기
	1. 허깅페이스는 토크나이저와 모델을 결합해 데이터의 전후처리와 모델 추론을 간단하게 수행하는 `pipeline`을 제공
2. 직접 모델과 토크나이저를 불러와서 활용하기