
1.  머신러닝: 데이터의 특징을 연구자 또는 개발자가 찾고 모델에 입력을 넣음
2. 딥러닝: 모델이 스스로 특징을 찾고 분류하는 모든 과정을 학습함
	1. 딥러닝 모델은 데이터의 의미를 숫자로 집합으로 표현
		 -> **임베딩** : 데이터의 의미와 특징을 포착해 숫자로 표현한 것

# 임베딩
데이터를 임베딩으로 표현하면 데이터 사이의 거리를 계산하고 거리를 바탕으로 관련 있는 데이터와 관련이 없는 데이터를 구분할 수 있음

거리를 계산할 수 있기 때문에 아래와 같은 작업에 활용 가능
1. 검색 및 추천
2. 클러스터링 및 분류
3. 이상치 탐지: 나머지 데이터와 거리가 먼 데이터는 이상치로 볼 수 있음

---
- RNN은 순차적인 처리 방식
- **어텐션 알고리즘**: 맥락을 모두 참조
- 2017년에 등장한 트랜스포머 아키텍처는 이 방식을 버리고 어텐션 연산을 사용
- -> 맥락을 압축하지 않고 그대로 사용하기 때문에 <u>성능 증가 but,  메모리 사용량 증가</u>

- OpenAI가 **지도 미세조정**과 **RLHF**(사람의 피드백을 통해 언어 모델이 지시를 따르도록 훈련하기)라는 기술을 공개
	- **지도 미세 조정**: 정렬을 위한 가장 핵심적인 학습 과정으로서, 언어 모델링으로 사전 학습한 언어 모델을 지시 데이터셋으로 추가 학습하는 것
	- 지시 데이터셧: 사용자가 요청 또는 지시한 사항-적절한 응답 을 정리한 데이터셋
	- **RLHF**: 두 가지 답변 중 사용자가 더 선호하는 답변을 선택한 데이터셋(선호 데이터셋)을 구축
		- 선호 데이터셋으로 LLM의 답변을 평가하는 리워드 모델을 만들고 LLM이 점점 더 높은 점수를 맞도록 추가 학습(이때, 강화학습을 사용)
- **정렬**: LLM이 생성하는 답변을 사용자의 요청 의도에 맞추는 것
 
---
# LLM 활용하는 방법
1. 상업용 API 사용
	1. 오픈소스 LLM에 비해 모델이 크고 범용 텍스트 생성 능력이 뛰어남
2. 오픈소스 LLM을 활용해 직접 LLM API를 생성해 사용하는 방법
	2. 원하는 도메인의 데이터, 작업을 위한 데이터로 자유롭게 추가 학습할 수 있음

-> 추가 학습을 하는 경우 모델 크기가 작으면서도 특정 도메인 데이터나 적업에서 높은 성능을 보이는 모델(**sLLM**)을 만들 수 있음
	많은 기업이 자신의 조직에 특화된 sLLM을 개발하기 위해 시도

GPU 자원으로도 LLM을 활용할 수 있도록 돕는 연구가 활발히 진행되고 있음
- 양자화: 모델 파라미터를 더 적은 비트로 표현하는 것
- **LoRA**(Low Rank Adaption): 모델 전체를 학습하는 것이 아닌 모델의 일부만 학습
