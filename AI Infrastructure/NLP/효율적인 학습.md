# 분산 학습
> GPU를 여러 개 활용해 딥러닝 모델을 학습시키는 것
- **데이터 병렬화**: 모델이 작아 하나의 GPU에 올릴 수 있는 경우 여러 GPU에 각각 모델을 올리고 학습 데이터를 병렬로 처리해 학습 속도를 높일 수 있음
- **모델 병렬화**: 하나의 GPU에 올리기 어려운 큰 모델의 경우 사용
	- 모델을 여러 개의 GPU에 나눠서 올리는 방식
	- **파이프라인 병렬화**: 딥러닝 모델의 층별로 나워 GPU에 올리는 방식
	- **텐서 병렬화**: 한 층의 모델도 나눠서 GPU에 올리는 방식
# LoRA
- 대부분의 개인과 조직은 여러 GPU를 사용해 모델을 학습시키기 어렵기 때문에 일부 파라미터만 학습하는 **PEFT** 방법 연구가 활발히 이뤄지고 있음
- 그중에서도 오픈소스 LLM 학습에서 가장 주목받고 많이 활용되는 학습 방법은 모델에 일부 파라미터를 추가하고 그 부분만 학습하는 LoRA 학습 방식
# KV 캐시
> 셀프 어텐션 연산 과정에서 동일한 입력 토큰에 대해 중복 계산이 발생하는 비효율을 줄이기 위해 먼저 계산했던 키와 값 결과를 메모리에 저장해 활용하는 방법